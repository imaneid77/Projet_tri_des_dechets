{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRI DE DECHETS**\n",
    "\n",
    "---\n",
    "\n",
    "R.Q. Ce notebook a été lancé sur google colab. L'importation des données a été faite via google drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I. DATA PRE PROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "from google.colab import drive\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# transformation & preprocessing\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as implt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# increasing data\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# training model & regularisation\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Model for transfer learning\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "drive.mount('/content/drive')\n",
    "image_base_path = '/content/drive/My Drive/dataset-master'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names\n",
    "class_names = ['crumpled_paper', 'disposable_paper_cups', 'egg_packaging', 'foil', 'glass_bottle', 'plastic_bottle', 'receipt']\n",
    "\n",
    "# Mapping des noms de classe à des entiers\n",
    "class_mapping = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "trash_images = []\n",
    "labels = []\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image_path = str(image_path)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, (128, 128))       # Taille fixe (par exemple, 128x128)\n",
    "    image = image.astype('float32') / 255.0     # Convertit l'image en tableau numpy et normaliser les valeurs de pixels\n",
    "    return image\n",
    "\n",
    "image_base_path = '/content/drive/My Drive/dataset-master'\n",
    "\n",
    "\n",
    "# Collecter les images et les étiquettes\n",
    "for label, trash_folder in enumerate(class_names):\n",
    "    trash_folder_path = os.path.join(image_base_path, trash_folder)\n",
    "    if os.path.isdir(trash_folder_path):\n",
    "        for image_file in os.listdir(trash_folder_path):\n",
    "            if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(trash_folder_path, image_file)\n",
    "                image = load_and_preprocess_image(image_path)\n",
    "                if image is not None:\n",
    "                    trash_images.append(image)\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Failed to load image: {image_path}\")  # Debugging statement\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {trash_folder_path}\")   # Debugging statement\n",
    "\n",
    "\n",
    "# Convertit les listes en tableaux numpy\n",
    "trash_images = np.array(trash_images)\n",
    "labels = np.array(labels).astype(np.int64)\n",
    "\n",
    "print(f\"Total images loaded: {len(trash_images)}\")  # Debugging statement\n",
    "\n",
    "# Check if any images were loaded\n",
    "if len(trash_images) == 0:\n",
    "    raise ValueError(\"No images were loaded. Please check your image path and data structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80% train + val, 20% test)\n",
    "train_val_images, test_images, train_val_labels, test_labels = train_test_split(trash_images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train-validation split (80% train, 20% val de l'ensemble train + val, donc 64% train, 16% (0.8*0.2) val du total)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_val_images, train_val_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encoding des labels\n",
    "train_labels = to_categorical(train_labels, num_classes=len(class_names))   #### train_val_labels ?\n",
    "val_labels = to_categorical(val_labels, num_classes=len(class_names))\n",
    "test_labels = to_categorical(test_labels, num_classes=len(class_names))\n",
    "\n",
    "print(f\"Total images: {len(trash_images)}\")\n",
    "print(f\"Train images: {len(train_images)}\")\n",
    "print(f\"Validation images: {len(val_images)}\")\n",
    "print(f\"Test images: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names\n",
    "def show_images(images, labels, class_names, num_images=10):\n",
    "    num_images = min(num_images, len(images))\n",
    "\n",
    "    plt.figure(figsize=(30, 20))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i])\n",
    "        label_index = np.argmax(labels[i])\n",
    "        plt.xlabel(class_names[label_index])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Affiche les 10 premières images d'entraînement et leurs étiquettes correspondantes\n",
    "show_images(train_val_images, train_labels, class_names, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **II. TRAINING AND EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. CNN modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():\n",
    "  \"\"\"\n",
    "  Cette fonction nous permet de créer un modèle CNN avec des couches personnalisées\n",
    "  \"\"\"\n",
    "\n",
    "  cnn_model = Sequential([\n",
    "      Conv2D(32, (3, 3), padding = 'same', activation='relu', input_shape=(128, 128, 3), kernel_regularizer=l2(0.05)),       #, kernel_regularizer=l2(0.01)\n",
    "      BatchNormalization(),\n",
    "      # Dropout(0.3),\n",
    "      MaxPooling2D((2, 2)),\n",
    "\n",
    "      Conv2D(64, (3, 3), padding = 'same', activation='relu', kernel_regularizer=l2(0.05)),                                  #, kernel_regularizer=l2(0.01)\n",
    "      BatchNormalization(),\n",
    "      # Dropout(0.3),\n",
    "      MaxPooling2D((2, 2)),\n",
    "\n",
    "      Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.05)),\n",
    "      BatchNormalization(),\n",
    "      MaxPooling2D((2, 2)),\n",
    "\n",
    "      Flatten(),\n",
    "      Dense(128, activation='relu', kernel_regularizer=l2(0.05)),\n",
    "\n",
    "      Dropout(0.5),   # 0.2\n",
    "      Dense(len(class_names), activation='softmax')\n",
    "  ])\n",
    "\n",
    "  return cnn_model\n",
    "\n",
    "cnn_model = build_cnn_model()\n",
    "\n",
    "# Initialise le modèle en lui passant une donnée\n",
    "cnn_model.predict(train_images[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile le modèle\n",
    "cnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Ajout de earlystopping, et reduce_lr afin de limiter l'overfitting\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=32)   #, restore_best_weights=True\n",
    "mc = ModelCheckpoint('cnn_model_best.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1) \n",
    "\n",
    "# Entraîne le modèle\n",
    "history = cnn_model.fit(train_images, train_labels, \n",
    "                        epochs=40, \n",
    "                        validation_data=(val_images, val_labels), \n",
    "                        batch_size=32, \n",
    "                        callbacks=[es, mc, reduce_lr])   # , reduce_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Les scores sont aux alentours de :  loss: 0.4917 - accuracy: 0.9709 - val_loss: 1.2728 - val_accuracy: 0.7297."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évalue le modèle sur les données de test\n",
    "test_loss, test_accuracy = cnn_model.evaluate(test_images, test_labels)\n",
    "print(\"Précision sur les données de test :\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va par la suite opter pour le transfer learning soit utiliser des modèles déjà pré-entraînés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. VGG19**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_model = keras.applications.VGG19(\n",
    "    input_shape=(128,128,3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg19_model.summary()\n",
    "\n",
    "# Gel des couches de base du modèle\n",
    "vgg19_model.trainable = False\n",
    "\n",
    "# Ajout des couches de classification\n",
    "x = vgg19_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)      ##\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.05))(x)   ##\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "# Crée le modèle final\n",
    "transf_vgg19_model = tf.keras.Model(vgg19_model.input, predictions)\n",
    "\n",
    "transf_vgg19_model.compile(optimizer='adam',              # tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "# Ajout de earlystopping, et reduce_lr afin de limiter l'overfitting\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=32)   #, restore_best_weights=True\n",
    "mc = ModelCheckpoint('cnn_model_best.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1) \n",
    "\n",
    "history = transf_vgg19_model.fit(train_images, train_labels, \n",
    "                                 validation_data=(val_images, val_labels), \n",
    "                                 batch_size=32, \n",
    "                                 epochs=40, \n",
    "                                 callbacks=[es, mc, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINE TUNE THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dégel des dernières couches du modèle de base\n",
    "vgg19_model.trainable = True     # transf_vgg19_model ou vgg19_model\n",
    "\n",
    "# Nombre de couches à dégeler\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Gèle toutes les couches avant 'fine_tune_at'\n",
    "for layer in vgg19_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compilation\n",
    "transf_vgg19_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),     # taux d'apprentissage plus faible\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_fine = transf_vgg19_model.fit(train_images, train_labels,\n",
    "                                  validation_data=(val_images, val_labels),\n",
    "                                  batch_size=32,\n",
    "                                  epochs=40,\n",
    "                                  callbacks=[es, mc])      # , reduce_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. EfficientNetb2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB2(\n",
    "    input_shape=(128,128,3),\n",
    "    include_top=False,        # on va définir des couches personnalisées ci après\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gel des couches de base du modèle\n",
    "base_model.trainable = False\n",
    "\n",
    "# Ajout de couches personnalisées de classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)                # , kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "# x = Dropout(0.3)(x)\n",
    "predictions = Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "# Crée le modèle final\n",
    "transf_model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Add earlystopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=32)    # , restore_best_weights=True\n",
    "mc = ModelCheckpoint('efficientnetb2_model_best.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "# Training\n",
    "history = transf_model.fit(train_images, train_labels, \n",
    "                           validation_data=(val_images, val_labels), \n",
    "                           batch_size=32, \n",
    "                           epochs=40, \n",
    "                           callbacks=[es, mc])      # , reduce_lr\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = transf_model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ce modèle est beaucoup moins performant que les autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **III. MAKE PREDICTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. CNN MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle enregistré si l'entrainement a déjà ete fait\n",
    "cnn_model_best = tf.keras.models.load_model('/content/cnn_model_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn_model.predict(test_images)\n",
    "\n",
    "# Predicted label pour la 1ère image\n",
    "predictions[0]\n",
    "\n",
    "# Classe qui a le taux de confiance le plus élevé\n",
    "prediction = np.argmax(predictions[0])\n",
    "print(class_names[prediction])\n",
    "\n",
    "# pour vérifier si la prédiction est correcte\n",
    "print(\"Label of this image is:\",class_names[np.argmax(test_labels[0])])\n",
    "plt.imshow(test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pour afficher les prédictions avec la probabilité de confiance\n",
    "def plot_image_prediction(i, predictions_array, class_names, true_label, img):\n",
    "  \n",
    "  predictions_array, true_label, img = predictions_array[i], np.argmax(true_label[i]), img[i]\n",
    "\n",
    "  plt.imshow(img)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "\n",
    "\n",
    "def plot_value_prediction(i, predictions_array, class_names, true_label):\n",
    "  \n",
    "  predictions_array, true_label = predictions_array[i], np.argmax(true_label[i])\n",
    "  thisplot = plt.bar(range(len(class_names)), predictions_array, color=\"#777777\")\n",
    "\n",
    "  plt.ylim([0, 1])\n",
    "  \n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')\n",
    "\n",
    "  plt.xticks(range(len(class_names)), \n",
    "             class_names, \n",
    "             rotation=45, \n",
    "             ha='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour voir les prédictions du modèle\n",
    "#@title Changer le slider pour observer les prédictions du modèle CNN classique! { run: \"auto\" }\n",
    "\n",
    "image_index = 96 #@param {type:\"slider\", min:0, max:100, step:1}\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plot_image_prediction(image_index, predictions, class_names, test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_prediction(image_index, predictions, class_names, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the first X test images, their predicted label, and the true label\n",
    "# Color correct predictions in blue, incorrect predictions in red\n",
    "num_rows = 5\n",
    "num_cols = 4\n",
    "num_images = num_rows*num_cols\n",
    "\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image_prediction(i, predictions, class_names,test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_prediction(i, predictions, class_names,test_labels)\n",
    "  plt.tight_layout(pad=2.0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
